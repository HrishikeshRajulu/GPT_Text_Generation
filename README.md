# GPT_Text_Generation_Notebook

## Description
This Jupyter notebook, `gpt.ipynb`, focuses on training a custom Generative Pretrained Transformer (GPT) model for text generation, using the "Tiny Shakespeare" dataset. The notebook showcases how to build and train a transformer-based language model from scratch, demonstrating key concepts like multi-head self-attention, layer normalization, and feedforward networks within the GPT architecture.

## Requirements
- Python 3.x
- PyTorch
- Jupyter (Lab or Notebook)

## Dataset
The "Tiny Shakespeare" dataset is used, which is a collection of Shakespeare's texts. The notebook includes a script to download this dataset directly.

## Installation
Install the required libraries using:
```bash
pip install torch
pip install jupyterlab  # or jupyter
